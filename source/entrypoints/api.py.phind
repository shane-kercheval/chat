import os
import threading
import queue
import uvicorn
from fastapi import FastAPI, Response
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)
from langchain.agents import initialize_agent
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)
from fastapi import FastAPI
from fastapi.responses import Response, StreamingResponse
from langchain import OpenAI
from langchain.agents import AgentType
from langchain.memory import ConversationBufferMemory
from langchain import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

app = FastAPI(
    title="Langchain AI API",
)

@app.on_event("startup")
async def startup():
    print("Server Startup!")

class ThreadedGenerator:
    def __init__(self):
        self.queue = queue.Queue()
    def __iter__(self):
        return self
    def next(self):
        item = self.queue.get()
        if item is StopIteration: raise item
        return item
    def send(self, data):
        self.queue.put(data)
    def close(self):
        self.queue.put(StopIteration)

class ChainStreamHandler(StreamingStdOutCallbackHandler):
    def __init__(self, gen):
        super().__init__()
        self.gen = gen
    def on_llm_new_token(self, token, **kwargs):
        self.gen.send(token)

def llm_thread(g, prompt):
    try:
        chat = ChatOpenAI(
            verbose=True,
            callback_manager=CallbackManager([ChainStreamHandler(g)]),
            temperature=0.7,
        )
        chat([SystemMessage(content="You are a poetic assistant"), HumanMessage(content=prompt)])
    finally:
        g.close()

def chat(prompt):
    g = ThreadedGenerator()
    threading.Thread(target=llm_thread, args=(g, prompt)).start()
    return g

@app.get("/stream")
async def stream():
    return StreamingResponse(chat("Hello"), media_type='text/event-stream')

def start():
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)